use std::{
    env,
    fmt::{self, Display},
    io::Read,
    num::NonZeroU32,
    path::PathBuf,
    str::FromStr,
};

use clap::{ArgGroup, Args, Parser, Subcommand};
use jemallocator::Jemalloc;
use lading::{
    blackhole,
    captures::CaptureManager,
    config::{Config, Telemetry},
    generator, inspector, observer,
    target::{self, Behavior, Output},
    target_metrics,
};
use metrics::gauge;
use metrics_exporter_prometheus::PrometheusBuilder;
use once_cell::sync::Lazy;
use regex::Regex;
use rustc_hash::FxHashMap;
use tokio::{
    runtime::Builder,
    signal,
    sync::broadcast,
    time::{self, Duration, sleep},
};
use tracing::{Instrument, debug, error, info, info_span, warn};
use tracing_subscriber::{EnvFilter, util::SubscriberInitExt};

#[global_allocator]
static GLOBAL: Jemalloc = Jemalloc;

#[derive(thiserror::Error, Debug)]
enum Error {
    #[error("Target related error: {0}")]
    Target(target::Error),
    #[error(transparent)]
    Io(#[from] std::io::Error),
    #[error("Lading generator returned an error: {0}")]
    LadingGenerator(#[from] lading::generator::Error),
    #[error("Lading blackhole returned an error: {0}")]
    LadingBlackhole(#[from] lading::blackhole::Error),
    #[error("Lading inspector returned an error: {0}")]
    LadingInspector(#[from] lading::inspector::Error),
    #[error("Lading observer returned an error: {0}")]
    LadingObserver(#[from] lading::observer::Error),
    #[error("Failed to deserialize Lading config: {0}")]
    SerdeYaml(#[from] serde_yaml::Error),
    #[error("Lading failed to sync servers {0}")]
    Send(#[from] tokio::sync::broadcast::error::SendError<Option<i32>>),
    #[error("Parsing Prometheus address failed: {0}")]
    PrometheusAddr(#[from] std::net::AddrParseError),
    #[error("Invalid capture path")]
    CapturePath,
    #[error("Invalid path for prometheus socket")]
    PrometheusPath,
    #[error(transparent)]
    Registration(#[from] lading_signal::RegisterError),
}

fn default_config_path() -> String {
    "/etc/lading/lading.yaml".to_string()
}

fn default_target_behavior() -> Behavior {
    Behavior::Quiet
}

#[derive(Default, Clone)]
struct CliKeyValues {
    inner: FxHashMap<String, String>,
}

impl CliKeyValues {
    #[cfg(test)]
    fn get(&self, key: &str) -> Option<&str> {
        self.inner.get(key).map(|s| s.as_str())
    }
}

impl Display for CliKeyValues {
    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> Result<(), fmt::Error> {
        for (k, v) in self.inner.iter() {
            write!(f, "{k}={v},")?;
        }
        Ok(())
    }
}

impl FromStr for CliKeyValues {
    type Err = String;

    fn from_str(input: &str) -> Result<Self, Self::Err> {
        // A key always matches `[[:alpha:]_]+` and a value conforms to
        // `[[:alpha:]_:,`. A key is always followed by a '=' and then a
        // value. A key and value pair are delimited from other pairs by
        // ','. But note that ',' is a valid character in a value, so it's
        // ambiguous whether the last member of a value after a ',' is a key.
        //
        // The approach taken here is to use the key notion as delimiter and
        // then tidy up afterward to find values.
        static RE: Lazy<Regex> =
            Lazy::new(|| Regex::new(r"([[:alpha:]_]+)=").expect("Invalid regex pattern provided"));

        let mut labels = FxHashMap::default();

        for cap in RE.captures_iter(input) {
            let key = cap[1].to_string();
            let start = cap.get(0).expect("value 0 not found in Captures").end();

            // Find the next key or run into the end of the input.
            let end = RE.find_at(input, start).map_or(input.len(), |m| m.start());

            // Extract the value.
            let value = input[start..end].trim_end_matches(',').to_string();

            labels.insert(key, value);
        }

        Ok(Self { inner: labels })
    }
}

// Parser for subcommand structure
#[derive(Parser)]
#[clap(version, about, long_about = None)]
struct CliWithSubcommands {
    #[command(subcommand)]
    command: Commands,
}

// Parser for legacy flat structure (deprecated)
#[derive(Parser)]
#[clap(version, about, long_about = None)]
struct CliFlatLegacy {
    #[command(flatten)]
    args: LadingArgs,
}

// Shared arguments used by both modes
#[derive(clap::Args)]
#[clap(group(
    ArgGroup::new("target")
        .required(true)
        .args(&["target_path", "target_pid", "target_container", "no_target"]),
))]
#[clap(group(
    ArgGroup::new("telemetry")
        .required(true)
        .args(&["capture_path", "prometheus_addr", "prometheus_path"]),
))]
#[clap(group(
     ArgGroup::new("experiment-duration")
           .required(false)
           .args(&["experiment_duration_seconds", "experiment_duration_infinite"]),
))]
struct LadingArgs {
    /// path on disk to the configuration file
    #[clap(long, default_value_t = default_config_path())]
    config_path: String,
    /// additional labels to apply to all captures, format KEY=VAL,KEY2=VAL
    #[clap(long)]
    global_labels: Option<CliKeyValues>,
    /// measure an externally-launched process by PID
    #[clap(long)]
    target_pid: Option<NonZeroU32>,
    /// measure an externally-launched container by name
    #[clap(long)]
    target_container: Option<String>,
    /// disable target measurement
    #[clap(long)]
    no_target: bool,
    /// the path of the target executable
    #[clap(long, group = "binary-target")]
    target_path: Option<PathBuf>,
    /// inherit the target environment variables from lading's environment
    #[clap(long, requires = "binary-target", action)]
    target_inherit_environment: bool,
    /// additional environment variables to apply to the target, format
    /// KEY=VAL,KEY2=VAL
    #[clap(long, requires = "binary-target")]
    target_environment_variables: Option<CliKeyValues>,
    /// arguments for the target executable
    #[clap(requires = "binary-target")]
    target_arguments: Vec<String>,
    /// the path to write target's stdout
    #[clap(long, default_value_t = default_target_behavior(), requires = "binary-target")]
    target_stdout_path: Behavior,
    /// the path to write target's stderr
    #[clap(long, default_value_t = default_target_behavior(), requires = "binary-target")]
    target_stderr_path: Behavior,
    /// path on disk to write captures, exclusive of prometheus-path and
    /// prometheus-addr
    #[clap(long)]
    capture_path: Option<String>,
    /// time that capture metrics will expire by if they are not seen again, only useful when capture-path is set
    #[clap(long)]
    capture_expiriation_seconds: Option<u64>,
    /// address to bind prometheus exporter to, exclusive of prometheus-path and
    /// promtheus-addr
    #[clap(long)]
    prometheus_path: Option<String>,
    /// socket to bind prometheus exporter to, exclusive of prometheus-addr and
    /// capture-path
    #[clap(long)]
    prometheus_addr: Option<String>,
    /// the maximum time to wait, in seconds, for controlled shutdown
    #[clap(long, default_value_t = 30)]
    max_shutdown_delay: u16,
    /// the time, in seconds, to run the target and collect samples about it
    #[clap(long, default_value_t = 120)]
    experiment_duration_seconds: u32,
    /// flag to allow infinite experiment duration
    #[clap(long)]
    experiment_duration_infinite: bool,
    /// the time, in seconds, to allow the target to run without collecting
    /// samples
    #[clap(long, default_value_t = 30)]
    warmup_duration_seconds: u32,
    /// whether to ignore inspector configuration, if present, and not run the inspector
    #[clap(long)]
    disable_inspector: bool,
}

#[derive(Subcommand)]
enum Commands {
    /// Run lading with specified configuration
    Run(Box<RunCommand>),
    /// Validate configuration file and exit
    ConfigCheck(ConfigCheckCommand),
}

#[derive(Args)]
struct RunCommand {
    #[command(flatten)]
    args: LadingArgs,
}

#[derive(Args)]
struct ConfigCheckCommand {
    /// path on disk to the configuration file
    #[clap(long, default_value_t = default_config_path())]
    config_path: String,
}

fn load_config_contents(config_path: &str) -> Result<String, Error> {
    if let Ok(env_var_value) = env::var("LADING_CONFIG") {
        debug!("Using config from env var 'LADING_CONFIG'");
        Ok(env_var_value)
    } else {
        debug!("Attempting to open configuration file at: {}", config_path);
        let mut file = std::fs::OpenOptions::new()
            .read(true)
            .open(config_path)
            .map_err(|err| {
                error!("Could not read config file '{}': {}", config_path, err);
                err
            })?;
        let mut contents = String::new();
        file.read_to_string(&mut contents)?;
        Ok(contents)
    }
}

fn parse_config(contents: &str) -> Result<Config, Error> {
    serde_yaml::from_str(contents).map_err(|err| {
        error!("Configuration validation failed: {}", err);
        Error::SerdeYaml(err)
    })
}

fn validate_config(config_path: &str) -> Result<Config, Error> {
    let contents = load_config_contents(config_path)?;
    let config = parse_config(&contents)?;
    info!("Configuration file is valid");
    Ok(config)
}

fn get_config(args: &LadingArgs, config: Option<String>) -> Result<Config, Error> {
    let contents = if let Some(config) = config {
        config
    } else {
        load_config_contents(&args.config_path)?
    };

    let mut config = parse_config(&contents)?;

    let target = if args.no_target {
        None
    } else if let Some(pid) = args.target_pid {
        Some(target::Config::Pid(target::PidConfig {
            pid: pid.try_into().expect("Could not convert pid to i32"),
        }))
    } else if let Some(name) = &args.target_container {
        Some(target::Config::Docker(target::DockerConfig {
            name: name.clone(),
        }))
    } else if let Some(path) = &args.target_path {
        Some(target::Config::Binary(target::BinaryConfig {
            command: path.clone(),
            arguments: args.target_arguments.clone(),
            inherit_environment: args.target_inherit_environment,
            environment_variables: args
                .target_environment_variables
                .clone()
                .unwrap_or_default()
                .inner,
            output: Output {
                stderr: args.target_stderr_path.clone(),
                stdout: args.target_stdout_path.clone(),
            },
        }))
    } else {
        unreachable!("clap ensures that exactly one target option is selected");
    };
    config.target = target;

    let options_global_labels = args.global_labels.clone().unwrap_or_default();
    if let Some(ref prom_addr) = args.prometheus_addr {
        config.telemetry = Telemetry::Prometheus {
            addr: prom_addr.parse()?,
            global_labels: options_global_labels.inner,
        };
    } else if let Some(ref prom_path) = args.prometheus_path {
        config.telemetry = Telemetry::PrometheusSocket {
            path: prom_path.parse().map_err(|_| Error::PrometheusPath)?,
            global_labels: options_global_labels.inner,
        };
    } else if let Some(ref capture_path) = args.capture_path {
        config.telemetry = Telemetry::Log {
            path: capture_path.parse().map_err(|_| Error::CapturePath)?,
            global_labels: options_global_labels.inner,
            expiration: Duration::from_secs(args.capture_expiriation_seconds.unwrap_or(u64::MAX)),
        };
    } else {
        match config.telemetry {
            Telemetry::Prometheus {
                ref mut global_labels,
                ..
            } => {
                for (k, v) in options_global_labels.inner {
                    global_labels.insert(k, v);
                }
            }
            Telemetry::PrometheusSocket {
                ref mut global_labels,
                ..
            } => {
                for (k, v) in options_global_labels.inner {
                    global_labels.insert(k, v);
                }
            }
            Telemetry::Log {
                ref mut global_labels,
                ..
            } => {
                for (k, v) in options_global_labels.inner {
                    global_labels.insert(k, v);
                }
            }
        }
    }
    Ok(config)
}

async fn inner_main(
    experiment_duration: Duration,
    warmup_duration: Duration,
    disable_inspector: bool,
    config: Config,
) -> Result<(), Error> {
    let (shutdown_watcher, shutdown_broadcast) = lading_signal::signal();
    let (experiment_started_watcher, experiment_started_broadcast) = lading_signal::signal();
    let (target_running_watcher, target_running_broadcast) = lading_signal::signal();

    // Set up the telemetry sub-system.
    //
    // We support two methods to exflitrate telemetry about the target from rig:
    // a passive prometheus export and an active log file. Only one can be
    // active at a time.
    match config.telemetry {
        Telemetry::PrometheusSocket {
            path,
            global_labels,
        } => {
            let mut builder = PrometheusBuilder::new().with_http_uds_listener(path);
            for (k, v) in global_labels {
                builder = builder.add_global_label(k, v);
            }
            tokio::spawn(async move {
                builder
                    .install()
                    .expect("failed to install prometheus recorder");
            });
        }
        Telemetry::Prometheus {
            addr,
            global_labels,
        } => {
            let mut builder = PrometheusBuilder::new().with_http_listener(addr);
            for (k, v) in global_labels {
                builder = builder.add_global_label(k, v);
            }
            tokio::spawn(async move {
                builder
                    .install()
                    .expect("failed to install prometheus recorder");
            });
        }
        Telemetry::Log {
            path,
            global_labels,
            expiration,
        } => {
            let mut capture_manager = CaptureManager::new(
                path,
                shutdown_watcher.register()?,
                experiment_started_watcher.clone(),
                target_running_watcher.clone(),
                expiration,
            )
            .await?;
            for (k, v) in global_labels {
                capture_manager.add_global_label(k, v);
            }
            tokio::spawn(async {
                capture_manager
                    .start()
                    .expect("failed to start capture manager");
            });
        }
    }

    // Set up the application servers. These are, depending on configuration:
    //
    // * the "generator" which pushes load into
    // * the "target" which is the measured system and might push load into
    // * the "blackhole" which may or may not exist.
    //
    // There is also, maybe:
    //
    // * the "inspector" which is a sub-process that users can rig to inspect
    //   the target.
    // * the "observer" which reads procfs on Linux and reports relevant process
    //   detail to the capture log

    let (tgt_snd, _tgt_rcv) = broadcast::channel(16); // Increased capacity to handle multiple generators + observer + inspector

    let mut gsrv_joinset = tokio::task::JoinSet::new();
    //
    // GENERATOR
    //
    for cfg in config.generator {
        let tgt_rcv = tgt_snd.subscribe();
        let generator_server = generator::Server::new(cfg, shutdown_watcher.clone())?;
        gsrv_joinset.spawn(generator_server.run(tgt_rcv));
    }

    //
    // INSPECTOR
    //
    if let Some(inspector_conf) = config.inspector {
        if !disable_inspector {
            let tgt_rcv = tgt_snd.subscribe();
            let inspector_server =
                inspector::Server::new(inspector_conf, shutdown_watcher.clone())?;
            let _isrv = tokio::spawn(inspector_server.run(tgt_rcv));
        }
    }

    //
    // BLACKHOLE
    //
    if let Some(cfgs) = config.blackhole {
        for cfg in cfgs {
            let blackhole_server = blackhole::Server::new(cfg, shutdown_watcher.clone())?;
            let _bsrv = tokio::spawn(async {
                match blackhole_server.run().await {
                    Ok(()) => debug!("blackhole shut down successfully"),
                    Err(err) => warn!("blackhole failed with {:?}", err),
                }
            });
        }
    }

    //
    // TARGET METRICS
    //
    if let Some(cfgs) = config.target_metrics {
        let sample_period = Duration::from_millis(config.sample_period_milliseconds);

        for cfg in cfgs {
            let metrics_server = target_metrics::Server::new(
                cfg,
                shutdown_watcher.clone(),
                experiment_started_watcher.clone(),
                sample_period,
            );
            tokio::spawn(async {
                match metrics_server.run().await {
                    Ok(()) => debug!("target_metrics shut down successfully"),
                    Err(err) => warn!("target_metrics failed with {:?}", err),
                }
            });
        }
    }

    let mut tsrv_joinset = tokio::task::JoinSet::new();
    let mut osrv_joinset = tokio::task::JoinSet::new();
    //
    // OBSERVER
    //
    // Observer is not used when there is no target.
    if let Some(target) = config.target {
        let obs_rcv = tgt_snd.subscribe();
        let observer_server = observer::Server::new(config.observer, shutdown_watcher.clone())?;
        let sample_period = Duration::from_millis(config.sample_period_milliseconds);
        osrv_joinset.spawn(observer_server.run(obs_rcv, sample_period));

        //
        // TARGET
        //
        let target_server = target::Server::new(target, shutdown_watcher.clone());
        tsrv_joinset.spawn(target_server.run(tgt_snd, target_running_broadcast));
    } else {
        // Many lading servers synchronize on target startup using the PID sender. Some by necessity, others by legacy.
        tgt_snd.send(None)?;
        // Newer usage prefers the `target_running` signal where the PID isn't needed.
        target_running_broadcast.signal();
    };

    let (timer_watcher, timer_broadcast) = lading_signal::signal();

    tokio::spawn(
        async move {
            info!("waiting for target startup");
            target_running_watcher.recv().await;
            info!("target is running, now sleeping for warmup");
            sleep(warmup_duration).await;
            experiment_started_broadcast.signal();
            info!("warmup completed, collecting samples");
            sleep(experiment_duration).await;
            info!("experiment duration exceeded, signaling for shutdown");
            timer_broadcast.signal();
        }
        .instrument(info_span!("experiment_sequence")),
    );

    // We must be sure to drop any unused watcher at this point. Below in
    // `signal_and_wait` if a watcher remains derived from `shutdown_watcher` the run will not shut down.
    drop(shutdown_watcher);
    drop(experiment_started_watcher);
    let timer_watcher_wait = timer_watcher.recv();
    tokio::pin!(timer_watcher_wait);
    let mut interval = time::interval(Duration::from_millis(400));
    let res = loop {
        tokio::select! {
            _ = interval.tick() => {
                gauge!("lading.running").set(1.0);
            },

            _ = signal::ctrl_c() => {
                info!("received ctrl-c");
                break Ok(());
            },
            _ = &mut timer_watcher_wait => {
                info!("shutdown signal received.");
                break Ok(());
            }
            Some(res) = osrv_joinset.join_next() => {
                match res {
                    Ok(observer_result) => match observer_result {
                        Ok(()) => { /* Observer shut down successfully */ }
                        Err(err) => {
                            error!("Observer shut down unexpectedly: {err}");
                            break Err(Error::LadingObserver(err));
                        }
                    }
                    Err(err) => error!("Could not join the spawned observer task: {}", err),
                }
            },
            Some(res) = gsrv_joinset.join_next() => {
                match res {
                    Ok(generator_result) => match generator_result {
                        Ok(()) => { /* Generator shut down successfully */ }
                        Err(err) => {
                            error!("Generator shut down unexpectedly: {}", err);
                            break Err(Error::LadingGenerator(err));
                        }
                    }
                    Err(err) => error!("Could not join the spawned generator task: {}", err),
                }
            },
            Some(target_result) = tsrv_joinset.join_next() => {
                match target_result {
                    // joined successfully, but how did the target server exit?
                    Ok(target_result) => match target_result {
                        Ok(_) => {
                            debug!("Target shut down successfully");
                            break Ok(());
                        }
                        Err(err) => {
                            error!("Target shut down unexpectedly: {}", err);
                            break Err(Error::Target(err));
                        }
                    }
                    Err(err) => panic!("Could not join the spawned target task: {err}"),
                }
            },
        }
    };
    shutdown_broadcast.signal_and_wait().await;
    res
}

fn main() -> Result<(), Error> {
    tracing_subscriber::fmt()
        .with_env_filter(EnvFilter::from_default_env())
        .with_ansi(false)
        .finish()
        .init();

    let version = env!("CARGO_PKG_VERSION");
    info!("Starting lading {version} run.");

    // Two-parser fallback logic until CliFlatLegacy is removed
    let args = match CliWithSubcommands::try_parse() {
        Ok(cli) => match cli.command {
            Commands::Run(run_cmd) => run_cmd.args,
            Commands::ConfigCheck(config_check_cmd) => {
                // Handle config-check command
                match validate_config(&config_check_cmd.config_path) {
                    Ok(_) => std::process::exit(0),
                    Err(_) => std::process::exit(1),
                }
            }
        },
        Err(_) => {
            // Fall back to legacy parsing
            match CliFlatLegacy::try_parse() {
                Ok(legacy) => legacy.args,
                Err(err) => err.exit(),
            }
        }
    };

    let config = get_config(&args, None);

    let experiment_duration = if args.experiment_duration_infinite {
        Duration::MAX
    } else {
        Duration::from_secs(args.experiment_duration_seconds.into())
    };

    let warmup_duration = Duration::from_secs(args.warmup_duration_seconds.into());
    // The maximum shutdown delay is shared between `inner_main` and this
    // function, hence the divide by two.
    let max_shutdown_delay = Duration::from_secs(args.max_shutdown_delay.into());
    let disable_inspector = args.disable_inspector;

    let runtime = Builder::new_multi_thread()
        .enable_io()
        .enable_time()
        .build()?;
    let res = runtime.block_on(inner_main(
        experiment_duration,
        warmup_duration,
        disable_inspector,
        config?,
    ));
    // The splunk_hec generator spawns long running tasks that are not plugged
    // into the shutdown mechanism we have here. This is a bug and needs to be
    // addressed. However as a workaround we explicitly shutdown the
    // runtime. Even when the splunk_hec issue is addressed we'll continue this
    // practice as it's a reasonable safeguard.
    info!(
        "Shutting down runtime with a {} second delay. May leave orphaned tasks.",
        max_shutdown_delay.as_secs(),
    );
    runtime.shutdown_timeout(max_shutdown_delay);
    info!("Bye. :)");
    res
}

#[cfg(test)]
mod tests {
    //! Integration tests for lading main functionality
    //!
    //! These tests verify that lading can handle various scenarios including:
    //! - Basic single generator operation
    //! - Multiple generators synchronization (addresses "JoinHandle polled after completion" panic)
    //! - Multiple generators with observer enabled
    //! - Broadcast channel capacity verification

    use super::*;

    #[tokio::test(flavor = "multi_thread")]
    async fn inner_main_capture_has_data() {
        let contents = r#"
generator: []
"#;

        let tmp_dir = tempfile::tempdir().expect("directory could not be created");
        let capture_path = tmp_dir.path().join("capture");
        let capture_arg = format!("--capture-path={}", capture_path.display());

        let args = vec!["lading", "--no-target", capture_arg.as_str()];
        let legacy_cli = CliFlatLegacy::parse_from(args);
        let config = get_config(&legacy_cli.args, Some(contents.to_string()));
        let exit_code = inner_main(
            Duration::from_millis(2500),
            Duration::from_millis(5000),
            false,
            config.expect("Could not convert to valid Config"),
        )
        .await;

        assert!(exit_code.is_ok());

        let contents = std::fs::read_to_string(capture_path)
            .expect("File path does not already exist or does not contain valid utf-8");
        assert!(contents.rmatches("lading.running").count() > 5);
    }

    /// Test that demonstrates the original "JoinHandle polled after completion" panic
    /// This happened when multiple generators with broadcast capacity 1 caused some to lag
    #[tokio::test(flavor = "multi_thread")]
    async fn test_broadcast_lag_joinset_panic_reproduction() {
        use tokio::sync::broadcast;
        use tokio::task::JoinSet;

        // Reproduce the exact problematic setup: capacity 1 with multiple generators
        let (sender, _) = broadcast::channel::<Option<i32>>(1);
        let mut joinset = JoinSet::new();

        // Spawn 3 generator tasks (simulating the "more than one generator" condition)
        for i in 0..3 {
            let mut rx = sender.subscribe();
            joinset.spawn(async move {
                match rx.recv().await {
                    Ok(_) => {
                        // Normal generator would do work here
                        tokio::time::sleep(Duration::from_millis(50)).await;
                    }
                    Err(broadcast::error::RecvError::Lagged(_)) => {
                        // Original bug: lagged generators would just return, completing early
                        // This creates the race condition that causes the panic
                        return;
                    }
                    Err(_) => return,
                }
            });
        }

        // Send PID - with capacity 1, at least one generator will lag and complete early  
        sender.send(Some(1234)).unwrap();

        // Collect results - some tasks complete due to lag, others finish normally
        let mut results = Vec::new();
        while let Some(result) = joinset.join_next().await {
            results.push(result.unwrap());
        }

        // All tasks completed, but some finished early due to broadcast lag
        // In the original code, continuing to poll the now-empty joinset would panic
        assert_eq!(results.len(), 3);
        
        // This test passes because we don't poll after completion
        // But it demonstrates the conditions that caused the panic
    }

    #[test]
    fn cli_key_values_deserializes_empty_string_to_empty_set() {
        let val = "";
        let deser = CliKeyValues::from_str(val);
        let deser = deser
            .expect("String could not be converted into valid CliKeyValues")
            .to_string();
        assert_eq!("", deser);
    }

    #[test]
    fn cli_key_values_deserializes_kv_list() {
        let val = "first=one,second=two";
        let deser =
            CliKeyValues::from_str(val).expect("String cannot be converted into CliKeyValues");

        assert_eq!(
            deser.get("first").expect("Deser does not have key first"),
            "one"
        );
        assert_eq!(
            deser.get("second").expect("Deser does not have key second"),
            "two"
        );
    }

    #[test]
    fn cli_key_values_deserializes_trailing_comma_kv_list() {
        let val = "first=one,";
        let deser =
            CliKeyValues::from_str(val).expect("String cannot be converted into CliKeyValues");

        assert_eq!(
            deser.get("first").expect("Deser does not have key first"),
            "one"
        );
    }

    #[test]
    fn cli_key_values_deserializes_separated_value_kv_comma() {
        let val = "DD_API_KEY=00000001,DD_TELEMETRY_ENABLED=true,DD_TAGS=uqhwd:b2xiyw,hf9gy:uwcy04";
        let deser = CliKeyValues::from_str(val);
        let deser = deser.expect("String cannot be converted into CliKeyValues");

        println!("RESULT: {deser}");

        assert_eq!(
            deser
                .get("DD_API_KEY")
                .expect("DD_API_KEY is not a valid key for the map"),
            "00000001"
        );
        assert_eq!(
            deser
                .get("DD_TELEMETRY_ENABLED")
                .expect("DD_TELEMETRY_ENABLED is not a valid key for the map"),
            "true"
        );
        assert_eq!(
            deser
                .get("DD_TAGS")
                .expect("DD_TAGS is not a valid key for the map"),
            "uqhwd:b2xiyw,hf9gy:uwcy04"
        );
    }

    #[test]
    fn cli_key_values_deserializes_empty_string_to_empty_set() {
        let contents = r#"
generator:
  - inner:
      tcp:
        target_host: "127.0.0.1"
        target_port: 8080
        variant:
          http1:
            request_method: "GET"
            target_uri: "/"
            headers: {}
            maximum_requests_per_second: 1
            body: null
            parallel_connections: 1
  - inner:
      tcp:
        target_host: "127.0.0.1"
        target_port: 8081
        variant:
          http1:
            request_method: "GET"
            target_uri: "/"
            headers: {}
            maximum_requests_per_second: 1
            body: null
            parallel_connections: 1
  - inner:
      tcp:
        target_host: "127.0.0.1"
        target_port: 8082
        variant:
          http1:
            request_method: "GET"
            target_uri: "/"
            headers: {}
            maximum_requests_per_second: 1
            body: null
            parallel_connections: 1
"#;

        let tmp_dir = tempfile::tempdir().expect("directory could not be created");
        let capture_path = tmp_dir.path().join("capture");
        let capture_arg = format!("--capture-path={}", capture_path.display());

        let args = vec!["lading", "--no-target", capture_arg.as_str()];
        let legacy_cli = CliFlatLegacy::parse_from(args);
        let config = get_config(&legacy_cli.args, Some(contents.to_string()));

        // This test verifies that multiple generators can be spawned and synchronized
        // without causing the "JoinHandle polled after completion" panic that occurred
        // when the broadcast channel had insufficient capacity
        let exit_code = inner_main(
            Duration::from_millis(1500), // warmup
            Duration::from_millis(3000), // experiment duration - short to avoid connection errors
            false,
            config.expect("Could not convert to valid Config"),
        )
        .await;

        // The test passes if lading completes without panic
        // Before the fix, this would panic with "JoinHandle polled after completion"
        // because generators would complete at different times due to broadcast lag
        assert!(exit_code.is_ok());

        let contents = std::fs::read_to_string(capture_path)
            .expect("File path does not already exist or does not contain valid utf-8");
        // Verify lading ran and produced metrics
        assert!(contents.rmatches("lading.running").count() > 2);
    }

    #[tokio::test(flavor = "multi_thread")]
    #[cfg(target_os = "linux")] // Observer only works on Linux
    async fn inner_main_multiple_generators_with_observer() {
        let contents = r#"
target:
  binary:
    command: "/bin/sleep"
    arguments: ["30"]
generator:
  - inner:
      file_gen:
        path: "/tmp/test_output1"
        variant:
          raw:
            maximum_bytes_per_second: 100
            static_path: null
            variant: "random_ascii_words"
  - inner:
      file_gen:
        path: "/tmp/test_output2"
        variant:
          raw:
            maximum_bytes_per_second: 100
            static_path: null
            variant: "random_ascii_words"
  - inner:
      file_gen:
        path: "/tmp/test_output3"
        variant:
          raw:
            maximum_bytes_per_second: 100
            static_path: null
            variant: "random_ascii_words"
observer: {}
"#;

        let tmp_dir = tempfile::tempdir().expect("directory could not be created");
        let capture_path = tmp_dir.path().join("capture");
        let capture_arg = format!("--capture-path={}", capture_path.display());

        let args = vec!["lading", capture_arg.as_str()];
        let legacy_cli = CliFlatLegacy::parse_from(args);
        let config = get_config(&legacy_cli.args, Some(contents.to_string()));

        // This test specifically targets the race condition with multiple generators + observer
        // The observer and generators all subscribe to the same broadcast channel for target PID
        // Before the fix, this would frequently panic due to broadcast lag
        let exit_code = inner_main(
            Duration::from_millis(1000), // warmup
            Duration::from_millis(2500), // experiment duration
            false,
            config.expect("Could not convert to valid Config"),
        )
        .await;

        // The test passes if lading completes without panic
        assert!(exit_code.is_ok());

        let contents = std::fs::read_to_string(capture_path)
            .expect("File path does not already exist or does not contain valid utf-8");
        // Verify lading ran and produced metrics
        assert!(contents.rmatches("lading.running").count() > 2);
        // Also verify observer metrics were collected
        assert!(contents.contains("total_cpu_percentage") || contents.contains("memory_usage"));
    }

    #[tokio::test(flavor = "multi_thread")]
    async fn broadcast_channel_capacity_test() {
        use tokio::sync::broadcast;
        use futures::future;

        // This test verifies that the broadcast channel capacity is sufficient
        // for multiple generators without causing RecvError::Lagged

        // Test with the original problematic capacity of 1
        let (sender1, _) = broadcast::channel::<i32>(1);

        // Create multiple receivers (simulating generators + observer)
        let mut rx1 = sender1.subscribe();
        let mut rx2 = sender1.subscribe();
        let mut rx3 = sender1.subscribe();
        let mut rx4 = sender1.subscribe(); // observer

        // Send PID
        sender1.send(12345).unwrap();

        // With capacity 1, some receivers will lag
        let results = future::join_all(vec![
            tokio::spawn(async move { rx1.recv().await }),
            tokio::spawn(async move { rx2.recv().await }),
            tokio::spawn(async move { rx3.recv().await }),
            tokio::spawn(async move { rx4.recv().await }),
        ]).await;

        // With capacity 1, at least some should have lagged
        let lagged_count = results.iter()
            .map(|r| r.as_ref().unwrap())
            .filter(|r| matches!(r, Err(broadcast::error::RecvError::Lagged(_))))
            .count();

        // This demonstrates the original problem - with capacity 1, we get lag errors
        assert!(lagged_count > 0, "Expected some receivers to lag with capacity 1");

        // Now test with our fixed capacity of 16
        let (sender2, _) = broadcast::channel::<i32>(16);

        let mut rx1 = sender2.subscribe();
        let mut rx2 = sender2.subscribe();
        let mut rx3 = sender2.subscribe();
        let mut rx4 = sender2.subscribe();

        sender2.send(12345).unwrap();

        let results = future::join_all(vec![
            tokio::spawn(async move { rx1.recv().await }),
            tokio::spawn(async move { rx2.recv().await }),
            tokio::spawn(async move { rx3.recv().await }),
            tokio::spawn(async move { rx4.recv().await }),
        ]).await;

        // With capacity 16, all should succeed
        for result in results {
            let recv_result = result.unwrap();
            assert!(recv_result.is_ok(), "All receivers should succeed with sufficient capacity");
            assert_eq!(recv_result.unwrap(), 12345);
        }
    }

    #[test]
    fn test_broadcast_capacity_configuration() {
        // This test verifies the broadcast channel capacity we chose
        // is appropriate for the expected number of subscribers

        let expected_generators = 5; // Reasonable upper bound for most use cases
        let observer = 1;
        let inspector = 1;

        let total_subscribers = expected_generators + observer + inspector;
        let chosen_capacity = 16;

        // Our capacity should accommodate all expected subscribers
        assert!(chosen_capacity >= total_subscribers,
               "Broadcast capacity ({}) should be at least {} to handle {} generators + observer + inspector",
               chosen_capacity, total_subscribers, expected_generators);

        // Should also have some buffer room
        assert!(chosen_capacity > total_subscribers * 2,
               "Broadcast capacity should have buffer room for timing variations");
    }

    /// Regression test for the specific panic that was occurring:
    /// "thread 'tokio-runtime-worker' panicked at tokio-1.44.2/src/runtime/task/core.rs:378:22:
    ///  JoinHandle polled after completion"
    ///
    /// This panic occurred when multiple generators caused some to lag behind the PID broadcast,
    /// complete early, and then the main loop continued polling the empty JoinSet.
    #[test]
    fn regression_test_joinhandle_polling_panic_conditions() {
        // This documents the conditions that caused the original panic:

        // 1. Multiple generators (more than 1)
        let generator_configs = vec!["gen1", "gen2", "gen3"];
        assert!(generator_configs.len() > 1, "Need multiple generators to trigger the race");

        // 2. Broadcast channel with insufficient capacity
        let original_capacity = 1;
        let subscriber_count = generator_configs.len() + 1; // +1 for observer
        assert!(original_capacity < subscriber_count,
               "Original capacity ({}) was insufficient for {} subscribers",
               original_capacity, subscriber_count);

        // 3. The fixed capacity should prevent the issue
        let fixed_capacity = 16;
        assert!(fixed_capacity >= subscriber_count * 2,
               "Fixed capacity should comfortably handle all subscribers with buffer");
    }
}
